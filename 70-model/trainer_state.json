{
  "best_global_step": 11064,
  "best_metric": 0.6793805317275947,
  "best_model_checkpoint": "/kaggle/working/output/checkpoints/checkpoint-11064",
  "epoch": 4.0,
  "eval_steps": 500,
  "global_step": 11064,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.018076644974692697,
      "grad_norm": 2.6240875720977783,
      "learning_rate": 6.507004066877543e-07,
      "loss": 0.7055,
      "step": 50
    },
    {
      "epoch": 0.036153289949385395,
      "grad_norm": 1.2045408487319946,
      "learning_rate": 1.3285133303208314e-06,
      "loss": 0.7077,
      "step": 100
    },
    {
      "epoch": 0.05422993492407809,
      "grad_norm": 1.134640097618103,
      "learning_rate": 2.006326253953909e-06,
      "loss": 0.7018,
      "step": 150
    },
    {
      "epoch": 0.07230657989877079,
      "grad_norm": 1.5894488096237183,
      "learning_rate": 2.684139177586986e-06,
      "loss": 0.6965,
      "step": 200
    },
    {
      "epoch": 0.09038322487346348,
      "grad_norm": 1.0604605674743652,
      "learning_rate": 3.3619521012200633e-06,
      "loss": 0.6951,
      "step": 250
    },
    {
      "epoch": 0.10845986984815618,
      "grad_norm": 3.2702674865722656,
      "learning_rate": 4.039765024853141e-06,
      "loss": 0.6888,
      "step": 300
    },
    {
      "epoch": 0.1265365148228489,
      "grad_norm": 4.156518936157227,
      "learning_rate": 4.704021690013556e-06,
      "loss": 0.6614,
      "step": 350
    },
    {
      "epoch": 0.14461315979754158,
      "grad_norm": 5.847091197967529,
      "learning_rate": 5.368278355173972e-06,
      "loss": 0.6273,
      "step": 400
    },
    {
      "epoch": 0.16268980477223427,
      "grad_norm": 6.3044562339782715,
      "learning_rate": 6.046091278807049e-06,
      "loss": 0.6207,
      "step": 450
    },
    {
      "epoch": 0.18076644974692696,
      "grad_norm": 8.622562408447266,
      "learning_rate": 6.723904202440127e-06,
      "loss": 0.6202,
      "step": 500
    },
    {
      "epoch": 0.19884309472161968,
      "grad_norm": 7.28903341293335,
      "learning_rate": 7.4017171260732034e-06,
      "loss": 0.5903,
      "step": 550
    },
    {
      "epoch": 0.21691973969631237,
      "grad_norm": 7.00501012802124,
      "learning_rate": 8.079530049706282e-06,
      "loss": 0.5816,
      "step": 600
    },
    {
      "epoch": 0.23499638467100506,
      "grad_norm": 5.817705154418945,
      "learning_rate": 8.743786714866696e-06,
      "loss": 0.5909,
      "step": 650
    },
    {
      "epoch": 0.2530730296456978,
      "grad_norm": 6.739103317260742,
      "learning_rate": 9.421599638499775e-06,
      "loss": 0.5876,
      "step": 700
    },
    {
      "epoch": 0.27114967462039047,
      "grad_norm": 7.5120697021484375,
      "learning_rate": 1.0099412562132853e-05,
      "loss": 0.5838,
      "step": 750
    },
    {
      "epoch": 0.28922631959508316,
      "grad_norm": 6.119781970977783,
      "learning_rate": 1.077722548576593e-05,
      "loss": 0.5669,
      "step": 800
    },
    {
      "epoch": 0.30730296456977585,
      "grad_norm": 5.463294506072998,
      "learning_rate": 1.1455038409399006e-05,
      "loss": 0.5559,
      "step": 850
    },
    {
      "epoch": 0.32537960954446854,
      "grad_norm": 3.392700433731079,
      "learning_rate": 1.2132851333032084e-05,
      "loss": 0.5776,
      "step": 900
    },
    {
      "epoch": 0.34345625451916123,
      "grad_norm": 7.870866298675537,
      "learning_rate": 1.2810664256665161e-05,
      "loss": 0.5703,
      "step": 950
    },
    {
      "epoch": 0.3615328994938539,
      "grad_norm": 12.447687149047852,
      "learning_rate": 1.3488477180298239e-05,
      "loss": 0.5648,
      "step": 1000
    },
    {
      "epoch": 0.3796095444685466,
      "grad_norm": 8.197169303894043,
      "learning_rate": 1.4166290103931315e-05,
      "loss": 0.5422,
      "step": 1050
    },
    {
      "epoch": 0.39768618944323936,
      "grad_norm": 6.5439910888671875,
      "learning_rate": 1.4844103027564392e-05,
      "loss": 0.5585,
      "step": 1100
    },
    {
      "epoch": 0.41576283441793205,
      "grad_norm": 5.760229110717773,
      "learning_rate": 1.552191595119747e-05,
      "loss": 0.5636,
      "step": 1150
    },
    {
      "epoch": 0.43383947939262474,
      "grad_norm": 7.655853748321533,
      "learning_rate": 1.6199728874830546e-05,
      "loss": 0.5592,
      "step": 1200
    },
    {
      "epoch": 0.45191612436731743,
      "grad_norm": 4.462482452392578,
      "learning_rate": 1.6877541798463625e-05,
      "loss": 0.5618,
      "step": 1250
    },
    {
      "epoch": 0.4699927693420101,
      "grad_norm": 67.25511169433594,
      "learning_rate": 1.75553547220967e-05,
      "loss": 0.5468,
      "step": 1300
    },
    {
      "epoch": 0.4880694143167028,
      "grad_norm": 4.67038631439209,
      "learning_rate": 1.823316764572978e-05,
      "loss": 0.5537,
      "step": 1350
    },
    {
      "epoch": 0.5061460592913956,
      "grad_norm": 2.5112268924713135,
      "learning_rate": 1.8897424310890194e-05,
      "loss": 0.5407,
      "step": 1400
    },
    {
      "epoch": 0.5242227042660882,
      "grad_norm": 3.8823957443237305,
      "learning_rate": 1.9575237234523274e-05,
      "loss": 0.5607,
      "step": 1450
    },
    {
      "epoch": 0.5422993492407809,
      "grad_norm": 8.470215797424316,
      "learning_rate": 2.025305015815635e-05,
      "loss": 0.5395,
      "step": 1500
    },
    {
      "epoch": 0.5603759942154736,
      "grad_norm": 62.287132263183594,
      "learning_rate": 2.093086308178943e-05,
      "loss": 0.5553,
      "step": 1550
    },
    {
      "epoch": 0.5784526391901663,
      "grad_norm": 23.49710464477539,
      "learning_rate": 2.1608676005422505e-05,
      "loss": 0.5391,
      "step": 1600
    },
    {
      "epoch": 0.596529284164859,
      "grad_norm": 11.77750015258789,
      "learning_rate": 2.2286488929055584e-05,
      "loss": 0.5648,
      "step": 1650
    },
    {
      "epoch": 0.6146059291395517,
      "grad_norm": 3.976848840713501,
      "learning_rate": 2.2964301852688656e-05,
      "loss": 0.5318,
      "step": 1700
    },
    {
      "epoch": 0.6326825741142444,
      "grad_norm": 3.3867084980010986,
      "learning_rate": 2.3642114776321736e-05,
      "loss": 0.5221,
      "step": 1750
    },
    {
      "epoch": 0.6507592190889371,
      "grad_norm": 3.3406925201416016,
      "learning_rate": 2.431992769995481e-05,
      "loss": 0.5371,
      "step": 1800
    },
    {
      "epoch": 0.6688358640636298,
      "grad_norm": 5.244052886962891,
      "learning_rate": 2.499774062358789e-05,
      "loss": 0.5309,
      "step": 1850
    },
    {
      "epoch": 0.6869125090383225,
      "grad_norm": 6.8826680183410645,
      "learning_rate": 2.5675553547220967e-05,
      "loss": 0.5355,
      "step": 1900
    },
    {
      "epoch": 0.7049891540130152,
      "grad_norm": 4.570790767669678,
      "learning_rate": 2.6353366470854046e-05,
      "loss": 0.5299,
      "step": 1950
    },
    {
      "epoch": 0.7230657989877078,
      "grad_norm": 5.747658729553223,
      "learning_rate": 2.7031179394487122e-05,
      "loss": 0.5284,
      "step": 2000
    },
    {
      "epoch": 0.7411424439624006,
      "grad_norm": 2.086024761199951,
      "learning_rate": 2.769543605964754e-05,
      "loss": 0.5477,
      "step": 2050
    },
    {
      "epoch": 0.7592190889370932,
      "grad_norm": 11.598358154296875,
      "learning_rate": 2.8373248983280615e-05,
      "loss": 0.5249,
      "step": 2100
    },
    {
      "epoch": 0.777295733911786,
      "grad_norm": 5.679154396057129,
      "learning_rate": 2.9051061906913695e-05,
      "loss": 0.561,
      "step": 2150
    },
    {
      "epoch": 0.7953723788864787,
      "grad_norm": 5.032041072845459,
      "learning_rate": 2.9728874830546767e-05,
      "loss": 0.5622,
      "step": 2200
    },
    {
      "epoch": 0.8134490238611713,
      "grad_norm": 7.579895496368408,
      "learning_rate": 2.9954807933718305e-05,
      "loss": 0.5579,
      "step": 2250
    },
    {
      "epoch": 0.8315256688358641,
      "grad_norm": 3.65073299407959,
      "learning_rate": 2.9879487823248806e-05,
      "loss": 0.5148,
      "step": 2300
    },
    {
      "epoch": 0.8496023138105567,
      "grad_norm": 2.9088969230651855,
      "learning_rate": 2.9804167712779314e-05,
      "loss": 0.5349,
      "step": 2350
    },
    {
      "epoch": 0.8676789587852495,
      "grad_norm": 7.083131790161133,
      "learning_rate": 2.9728847602309815e-05,
      "loss": 0.5174,
      "step": 2400
    },
    {
      "epoch": 0.8857556037599421,
      "grad_norm": 6.748347282409668,
      "learning_rate": 2.9653527491840323e-05,
      "loss": 0.5311,
      "step": 2450
    },
    {
      "epoch": 0.9038322487346349,
      "grad_norm": 5.164144039154053,
      "learning_rate": 2.9578207381370828e-05,
      "loss": 0.5088,
      "step": 2500
    },
    {
      "epoch": 0.9219088937093276,
      "grad_norm": 4.392017364501953,
      "learning_rate": 2.950288727090133e-05,
      "loss": 0.5153,
      "step": 2550
    },
    {
      "epoch": 0.9399855386840202,
      "grad_norm": 5.763041019439697,
      "learning_rate": 2.9427567160431837e-05,
      "loss": 0.5107,
      "step": 2600
    },
    {
      "epoch": 0.958062183658713,
      "grad_norm": 5.535926818847656,
      "learning_rate": 2.935224704996234e-05,
      "loss": 0.5124,
      "step": 2650
    },
    {
      "epoch": 0.9761388286334056,
      "grad_norm": 4.987810134887695,
      "learning_rate": 2.9276926939492847e-05,
      "loss": 0.525,
      "step": 2700
    },
    {
      "epoch": 0.9942154736080984,
      "grad_norm": 5.654569149017334,
      "learning_rate": 2.920160682902335e-05,
      "loss": 0.5204,
      "step": 2750
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.6008808147536471,
      "eval_f1_macro": 0.5748061049149982,
      "eval_loss": 0.8683445453643799,
      "eval_runtime": 167.9669,
      "eval_samples_per_second": 64.888,
      "eval_steps_per_second": 4.06,
      "step": 2766
    },
    {
      "epoch": 1.0122921185827911,
      "grad_norm": 2.9145116806030273,
      "learning_rate": 2.9126286718553856e-05,
      "loss": 0.5107,
      "step": 2800
    },
    {
      "epoch": 1.0303687635574836,
      "grad_norm": 3.046647548675537,
      "learning_rate": 2.905096660808436e-05,
      "loss": 0.5058,
      "step": 2850
    },
    {
      "epoch": 1.0484454085321764,
      "grad_norm": 2.9413857460021973,
      "learning_rate": 2.897564649761486e-05,
      "loss": 0.4882,
      "step": 2900
    },
    {
      "epoch": 1.0665220535068691,
      "grad_norm": 1.9288996458053589,
      "learning_rate": 2.890032638714537e-05,
      "loss": 0.4901,
      "step": 2950
    },
    {
      "epoch": 1.0845986984815619,
      "grad_norm": 3.9485180377960205,
      "learning_rate": 2.8825006276675874e-05,
      "loss": 0.5028,
      "step": 3000
    },
    {
      "epoch": 1.1026753434562546,
      "grad_norm": 5.1181793212890625,
      "learning_rate": 2.874968616620638e-05,
      "loss": 0.5114,
      "step": 3050
    },
    {
      "epoch": 1.1207519884309471,
      "grad_norm": 3.204162359237671,
      "learning_rate": 2.8674366055736884e-05,
      "loss": 0.5005,
      "step": 3100
    },
    {
      "epoch": 1.13882863340564,
      "grad_norm": 3.6302623748779297,
      "learning_rate": 2.8599045945267385e-05,
      "loss": 0.4937,
      "step": 3150
    },
    {
      "epoch": 1.1569052783803326,
      "grad_norm": 3.6700024604797363,
      "learning_rate": 2.8523725834797893e-05,
      "loss": 0.4871,
      "step": 3200
    },
    {
      "epoch": 1.1749819233550254,
      "grad_norm": 5.397378444671631,
      "learning_rate": 2.8448405724328394e-05,
      "loss": 0.5029,
      "step": 3250
    },
    {
      "epoch": 1.1930585683297181,
      "grad_norm": 3.876650333404541,
      "learning_rate": 2.8373085613858902e-05,
      "loss": 0.4901,
      "step": 3300
    },
    {
      "epoch": 1.2111352133044107,
      "grad_norm": 7.403041362762451,
      "learning_rate": 2.8297765503389407e-05,
      "loss": 0.5018,
      "step": 3350
    },
    {
      "epoch": 1.2292118582791034,
      "grad_norm": 3.8464748859405518,
      "learning_rate": 2.8222445392919908e-05,
      "loss": 0.4995,
      "step": 3400
    },
    {
      "epoch": 1.2472885032537961,
      "grad_norm": 4.479097366333008,
      "learning_rate": 2.8147125282450416e-05,
      "loss": 0.4875,
      "step": 3450
    },
    {
      "epoch": 1.2653651482284887,
      "grad_norm": 3.3189051151275635,
      "learning_rate": 2.8071805171980917e-05,
      "loss": 0.4848,
      "step": 3500
    },
    {
      "epoch": 1.2834417932031814,
      "grad_norm": 3.6300601959228516,
      "learning_rate": 2.7996485061511425e-05,
      "loss": 0.4915,
      "step": 3550
    },
    {
      "epoch": 1.3015184381778742,
      "grad_norm": 4.414632797241211,
      "learning_rate": 2.792116495104193e-05,
      "loss": 0.4902,
      "step": 3600
    },
    {
      "epoch": 1.319595083152567,
      "grad_norm": 5.312897205352783,
      "learning_rate": 2.784584484057243e-05,
      "loss": 0.4994,
      "step": 3650
    },
    {
      "epoch": 1.3376717281272597,
      "grad_norm": 4.6328864097595215,
      "learning_rate": 2.777052473010294e-05,
      "loss": 0.4756,
      "step": 3700
    },
    {
      "epoch": 1.3557483731019522,
      "grad_norm": 3.8270297050476074,
      "learning_rate": 2.769520461963344e-05,
      "loss": 0.4888,
      "step": 3750
    },
    {
      "epoch": 1.373825018076645,
      "grad_norm": 2.4144723415374756,
      "learning_rate": 2.761988450916395e-05,
      "loss": 0.4853,
      "step": 3800
    },
    {
      "epoch": 1.3919016630513377,
      "grad_norm": 3.1766510009765625,
      "learning_rate": 2.7544564398694453e-05,
      "loss": 0.4868,
      "step": 3850
    },
    {
      "epoch": 1.4099783080260304,
      "grad_norm": 2.0679354667663574,
      "learning_rate": 2.7469244288224958e-05,
      "loss": 0.4569,
      "step": 3900
    },
    {
      "epoch": 1.4280549530007232,
      "grad_norm": 2.356900930404663,
      "learning_rate": 2.7393924177755462e-05,
      "loss": 0.4784,
      "step": 3950
    },
    {
      "epoch": 1.4461315979754157,
      "grad_norm": 4.673137664794922,
      "learning_rate": 2.7318604067285963e-05,
      "loss": 0.5007,
      "step": 4000
    },
    {
      "epoch": 1.4642082429501084,
      "grad_norm": 2.940927743911743,
      "learning_rate": 2.724328395681647e-05,
      "loss": 0.4744,
      "step": 4050
    },
    {
      "epoch": 1.4822848879248012,
      "grad_norm": 3.7873964309692383,
      "learning_rate": 2.7167963846346976e-05,
      "loss": 0.4785,
      "step": 4100
    },
    {
      "epoch": 1.5003615328994937,
      "grad_norm": 2.6710431575775146,
      "learning_rate": 2.709264373587748e-05,
      "loss": 0.4638,
      "step": 4150
    },
    {
      "epoch": 1.5184381778741867,
      "grad_norm": 5.8968095779418945,
      "learning_rate": 2.7017323625407985e-05,
      "loss": 0.485,
      "step": 4200
    },
    {
      "epoch": 1.5365148228488792,
      "grad_norm": 3.7915518283843994,
      "learning_rate": 2.6942003514938487e-05,
      "loss": 0.4455,
      "step": 4250
    },
    {
      "epoch": 1.554591467823572,
      "grad_norm": 3.3806910514831543,
      "learning_rate": 2.6866683404468995e-05,
      "loss": 0.4695,
      "step": 4300
    },
    {
      "epoch": 1.5726681127982647,
      "grad_norm": 3.4667298793792725,
      "learning_rate": 2.67913632939995e-05,
      "loss": 0.4561,
      "step": 4350
    },
    {
      "epoch": 1.5907447577729572,
      "grad_norm": 2.1790084838867188,
      "learning_rate": 2.6716043183530004e-05,
      "loss": 0.4729,
      "step": 4400
    },
    {
      "epoch": 1.6088214027476502,
      "grad_norm": 3.8874216079711914,
      "learning_rate": 2.664072307306051e-05,
      "loss": 0.462,
      "step": 4450
    },
    {
      "epoch": 1.6268980477223427,
      "grad_norm": 19.565439224243164,
      "learning_rate": 2.656540296259101e-05,
      "loss": 0.4551,
      "step": 4500
    },
    {
      "epoch": 1.6449746926970354,
      "grad_norm": 4.376842021942139,
      "learning_rate": 2.6490082852121518e-05,
      "loss": 0.4874,
      "step": 4550
    },
    {
      "epoch": 1.6630513376717282,
      "grad_norm": 3.923107862472534,
      "learning_rate": 2.6414762741652022e-05,
      "loss": 0.4632,
      "step": 4600
    },
    {
      "epoch": 1.6811279826464207,
      "grad_norm": 4.166396141052246,
      "learning_rate": 2.6339442631182527e-05,
      "loss": 0.4583,
      "step": 4650
    },
    {
      "epoch": 1.6992046276211137,
      "grad_norm": 5.573056697845459,
      "learning_rate": 2.626412252071303e-05,
      "loss": 0.4602,
      "step": 4700
    },
    {
      "epoch": 1.7172812725958062,
      "grad_norm": 3.8357951641082764,
      "learning_rate": 2.6188802410243533e-05,
      "loss": 0.4655,
      "step": 4750
    },
    {
      "epoch": 1.735357917570499,
      "grad_norm": 3.595266819000244,
      "learning_rate": 2.611348229977404e-05,
      "loss": 0.4661,
      "step": 4800
    },
    {
      "epoch": 1.7534345625451917,
      "grad_norm": 3.6316792964935303,
      "learning_rate": 2.6038162189304546e-05,
      "loss": 0.4766,
      "step": 4850
    },
    {
      "epoch": 1.7715112075198842,
      "grad_norm": 4.885561943054199,
      "learning_rate": 2.596284207883505e-05,
      "loss": 0.4556,
      "step": 4900
    },
    {
      "epoch": 1.789587852494577,
      "grad_norm": 2.8000636100769043,
      "learning_rate": 2.5887521968365555e-05,
      "loss": 0.4751,
      "step": 4950
    },
    {
      "epoch": 1.8076644974692697,
      "grad_norm": 5.20465087890625,
      "learning_rate": 2.5812201857896056e-05,
      "loss": 0.4569,
      "step": 5000
    },
    {
      "epoch": 1.8257411424439622,
      "grad_norm": 2.5931036472320557,
      "learning_rate": 2.5736881747426564e-05,
      "loss": 0.4694,
      "step": 5050
    },
    {
      "epoch": 1.8438177874186552,
      "grad_norm": 2.544837236404419,
      "learning_rate": 2.566156163695707e-05,
      "loss": 0.4711,
      "step": 5100
    },
    {
      "epoch": 1.8618944323933477,
      "grad_norm": 3.2684144973754883,
      "learning_rate": 2.5586241526487573e-05,
      "loss": 0.4661,
      "step": 5150
    },
    {
      "epoch": 1.8799710773680405,
      "grad_norm": 4.839717864990234,
      "learning_rate": 2.5510921416018078e-05,
      "loss": 0.4618,
      "step": 5200
    },
    {
      "epoch": 1.8980477223427332,
      "grad_norm": 2.379404067993164,
      "learning_rate": 2.5435601305548583e-05,
      "loss": 0.4679,
      "step": 5250
    },
    {
      "epoch": 1.9161243673174257,
      "grad_norm": 4.263820171356201,
      "learning_rate": 2.5360281195079087e-05,
      "loss": 0.4616,
      "step": 5300
    },
    {
      "epoch": 1.9342010122921187,
      "grad_norm": 8.178287506103516,
      "learning_rate": 2.5284961084609592e-05,
      "loss": 0.4528,
      "step": 5350
    },
    {
      "epoch": 1.9522776572668112,
      "grad_norm": 2.2297987937927246,
      "learning_rate": 2.5209640974140096e-05,
      "loss": 0.4506,
      "step": 5400
    },
    {
      "epoch": 1.970354302241504,
      "grad_norm": 5.024289608001709,
      "learning_rate": 2.51343208636706e-05,
      "loss": 0.4445,
      "step": 5450
    },
    {
      "epoch": 1.9884309472161967,
      "grad_norm": 5.587437152862549,
      "learning_rate": 2.5059000753201106e-05,
      "loss": 0.4579,
      "step": 5500
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.6483163592990182,
      "eval_f1_macro": 0.6417459793734865,
      "eval_loss": 0.8459280133247375,
      "eval_runtime": 167.5329,
      "eval_samples_per_second": 65.056,
      "eval_steps_per_second": 4.071,
      "step": 5532
    },
    {
      "epoch": 2.0065075921908893,
      "grad_norm": 4.09213924407959,
      "learning_rate": 2.498368064273161e-05,
      "loss": 0.4465,
      "step": 5550
    },
    {
      "epoch": 2.0245842371655822,
      "grad_norm": 3.142958879470825,
      "learning_rate": 2.490836053226211e-05,
      "loss": 0.4412,
      "step": 5600
    },
    {
      "epoch": 2.0426608821402747,
      "grad_norm": 2.5287370681762695,
      "learning_rate": 2.483304042179262e-05,
      "loss": 0.4418,
      "step": 5650
    },
    {
      "epoch": 2.0607375271149673,
      "grad_norm": 9.62479305267334,
      "learning_rate": 2.4757720311323124e-05,
      "loss": 0.4272,
      "step": 5700
    },
    {
      "epoch": 2.0788141720896602,
      "grad_norm": 5.653696060180664,
      "learning_rate": 2.468240020085363e-05,
      "loss": 0.4378,
      "step": 5750
    },
    {
      "epoch": 2.0968908170643528,
      "grad_norm": 2.9372947216033936,
      "learning_rate": 2.4607080090384134e-05,
      "loss": 0.4294,
      "step": 5800
    },
    {
      "epoch": 2.1149674620390457,
      "grad_norm": 3.2415738105773926,
      "learning_rate": 2.4531759979914635e-05,
      "loss": 0.4283,
      "step": 5850
    },
    {
      "epoch": 2.1330441070137383,
      "grad_norm": 2.9612646102905273,
      "learning_rate": 2.4456439869445143e-05,
      "loss": 0.4275,
      "step": 5900
    },
    {
      "epoch": 2.151120751988431,
      "grad_norm": 3.691120147705078,
      "learning_rate": 2.4381119758975647e-05,
      "loss": 0.4411,
      "step": 5950
    },
    {
      "epoch": 2.1691973969631237,
      "grad_norm": 2.781977415084839,
      "learning_rate": 2.4305799648506152e-05,
      "loss": 0.4324,
      "step": 6000
    },
    {
      "epoch": 2.1872740419378163,
      "grad_norm": 6.624584197998047,
      "learning_rate": 2.4230479538036657e-05,
      "loss": 0.4393,
      "step": 6050
    },
    {
      "epoch": 2.2053506869125092,
      "grad_norm": 4.7185959815979,
      "learning_rate": 2.4155159427567158e-05,
      "loss": 0.43,
      "step": 6100
    },
    {
      "epoch": 2.2234273318872018,
      "grad_norm": 3.027488946914673,
      "learning_rate": 2.4079839317097666e-05,
      "loss": 0.4405,
      "step": 6150
    },
    {
      "epoch": 2.2415039768618943,
      "grad_norm": 1.8243550062179565,
      "learning_rate": 2.400451920662817e-05,
      "loss": 0.4361,
      "step": 6200
    },
    {
      "epoch": 2.2595806218365873,
      "grad_norm": 5.371155261993408,
      "learning_rate": 2.3929199096158675e-05,
      "loss": 0.4298,
      "step": 6250
    },
    {
      "epoch": 2.27765726681128,
      "grad_norm": 7.011276721954346,
      "learning_rate": 2.385387898568918e-05,
      "loss": 0.433,
      "step": 6300
    },
    {
      "epoch": 2.2957339117859723,
      "grad_norm": 4.827812671661377,
      "learning_rate": 2.3778558875219684e-05,
      "loss": 0.4319,
      "step": 6350
    },
    {
      "epoch": 2.3138105567606653,
      "grad_norm": 3.477630138397217,
      "learning_rate": 2.370323876475019e-05,
      "loss": 0.4148,
      "step": 6400
    },
    {
      "epoch": 2.331887201735358,
      "grad_norm": 5.179015636444092,
      "learning_rate": 2.3627918654280694e-05,
      "loss": 0.443,
      "step": 6450
    },
    {
      "epoch": 2.3499638467100508,
      "grad_norm": 3.929607391357422,
      "learning_rate": 2.3552598543811198e-05,
      "loss": 0.4329,
      "step": 6500
    },
    {
      "epoch": 2.3680404916847433,
      "grad_norm": 2.413163423538208,
      "learning_rate": 2.3477278433341703e-05,
      "loss": 0.4162,
      "step": 6550
    },
    {
      "epoch": 2.3861171366594363,
      "grad_norm": 6.689938545227051,
      "learning_rate": 2.3401958322872208e-05,
      "loss": 0.4367,
      "step": 6600
    },
    {
      "epoch": 2.404193781634129,
      "grad_norm": 4.3309783935546875,
      "learning_rate": 2.3326638212402712e-05,
      "loss": 0.4064,
      "step": 6650
    },
    {
      "epoch": 2.4222704266088213,
      "grad_norm": 4.055209636688232,
      "learning_rate": 2.3251318101933217e-05,
      "loss": 0.4404,
      "step": 6700
    },
    {
      "epoch": 2.4403470715835143,
      "grad_norm": 5.576313018798828,
      "learning_rate": 2.317599799146372e-05,
      "loss": 0.42,
      "step": 6750
    },
    {
      "epoch": 2.458423716558207,
      "grad_norm": 1.7829471826553345,
      "learning_rate": 2.3100677880994226e-05,
      "loss": 0.4413,
      "step": 6800
    },
    {
      "epoch": 2.4765003615328993,
      "grad_norm": 2.827871799468994,
      "learning_rate": 2.302535777052473e-05,
      "loss": 0.4253,
      "step": 6850
    },
    {
      "epoch": 2.4945770065075923,
      "grad_norm": 8.216678619384766,
      "learning_rate": 2.2950037660055235e-05,
      "loss": 0.4331,
      "step": 6900
    },
    {
      "epoch": 2.512653651482285,
      "grad_norm": 2.5539498329162598,
      "learning_rate": 2.287471754958574e-05,
      "loss": 0.4215,
      "step": 6950
    },
    {
      "epoch": 2.5307302964569773,
      "grad_norm": 3.8005595207214355,
      "learning_rate": 2.2799397439116245e-05,
      "loss": 0.4361,
      "step": 7000
    },
    {
      "epoch": 2.5488069414316703,
      "grad_norm": 3.8392720222473145,
      "learning_rate": 2.272407732864675e-05,
      "loss": 0.4196,
      "step": 7050
    },
    {
      "epoch": 2.566883586406363,
      "grad_norm": 4.897860050201416,
      "learning_rate": 2.2648757218177254e-05,
      "loss": 0.4093,
      "step": 7100
    },
    {
      "epoch": 2.584960231381056,
      "grad_norm": 5.922501564025879,
      "learning_rate": 2.257343710770776e-05,
      "loss": 0.4275,
      "step": 7150
    },
    {
      "epoch": 2.6030368763557483,
      "grad_norm": 4.698699474334717,
      "learning_rate": 2.2498116997238263e-05,
      "loss": 0.4178,
      "step": 7200
    },
    {
      "epoch": 2.6211135213304413,
      "grad_norm": 3.5539302825927734,
      "learning_rate": 2.2422796886768768e-05,
      "loss": 0.4428,
      "step": 7250
    },
    {
      "epoch": 2.639190166305134,
      "grad_norm": 5.766083717346191,
      "learning_rate": 2.2347476776299272e-05,
      "loss": 0.4491,
      "step": 7300
    },
    {
      "epoch": 2.6572668112798263,
      "grad_norm": 4.257000923156738,
      "learning_rate": 2.2272156665829777e-05,
      "loss": 0.4276,
      "step": 7350
    },
    {
      "epoch": 2.6753434562545193,
      "grad_norm": 4.397918701171875,
      "learning_rate": 2.219683655536028e-05,
      "loss": 0.4025,
      "step": 7400
    },
    {
      "epoch": 2.693420101229212,
      "grad_norm": 10.54305362701416,
      "learning_rate": 2.212151644489079e-05,
      "loss": 0.4377,
      "step": 7450
    },
    {
      "epoch": 2.7114967462039044,
      "grad_norm": 4.323968887329102,
      "learning_rate": 2.204619633442129e-05,
      "loss": 0.4119,
      "step": 7500
    },
    {
      "epoch": 2.7295733911785973,
      "grad_norm": 3.7502782344818115,
      "learning_rate": 2.1970876223951795e-05,
      "loss": 0.4364,
      "step": 7550
    },
    {
      "epoch": 2.74765003615329,
      "grad_norm": 3.7700343132019043,
      "learning_rate": 2.18955561134823e-05,
      "loss": 0.4156,
      "step": 7600
    },
    {
      "epoch": 2.765726681127983,
      "grad_norm": 4.409424304962158,
      "learning_rate": 2.1820236003012805e-05,
      "loss": 0.4294,
      "step": 7650
    },
    {
      "epoch": 2.7838033261026753,
      "grad_norm": 8.541627883911133,
      "learning_rate": 2.1744915892543313e-05,
      "loss": 0.4288,
      "step": 7700
    },
    {
      "epoch": 2.8018799710773683,
      "grad_norm": 5.527329921722412,
      "learning_rate": 2.1669595782073814e-05,
      "loss": 0.4129,
      "step": 7750
    },
    {
      "epoch": 2.819956616052061,
      "grad_norm": 3.2673351764678955,
      "learning_rate": 2.159427567160432e-05,
      "loss": 0.4248,
      "step": 7800
    },
    {
      "epoch": 2.8380332610267534,
      "grad_norm": 3.8358614444732666,
      "learning_rate": 2.1518955561134823e-05,
      "loss": 0.4353,
      "step": 7850
    },
    {
      "epoch": 2.8561099060014463,
      "grad_norm": 8.893372535705566,
      "learning_rate": 2.1443635450665328e-05,
      "loss": 0.4139,
      "step": 7900
    },
    {
      "epoch": 2.874186550976139,
      "grad_norm": 9.143529891967773,
      "learning_rate": 2.1368315340195833e-05,
      "loss": 0.4243,
      "step": 7950
    },
    {
      "epoch": 2.8922631959508314,
      "grad_norm": 3.58361554145813,
      "learning_rate": 2.1292995229726337e-05,
      "loss": 0.4222,
      "step": 8000
    },
    {
      "epoch": 2.9103398409255243,
      "grad_norm": 5.054955959320068,
      "learning_rate": 2.1217675119256842e-05,
      "loss": 0.4186,
      "step": 8050
    },
    {
      "epoch": 2.928416485900217,
      "grad_norm": 5.2111616134643555,
      "learning_rate": 2.1142355008787346e-05,
      "loss": 0.3923,
      "step": 8100
    },
    {
      "epoch": 2.9464931308749094,
      "grad_norm": 3.663569927215576,
      "learning_rate": 2.106703489831785e-05,
      "loss": 0.4087,
      "step": 8150
    },
    {
      "epoch": 2.9645697758496024,
      "grad_norm": 3.825740337371826,
      "learning_rate": 2.0991714787848356e-05,
      "loss": 0.4365,
      "step": 8200
    },
    {
      "epoch": 2.982646420824295,
      "grad_norm": 3.1458609104156494,
      "learning_rate": 2.091639467737886e-05,
      "loss": 0.4151,
      "step": 8250
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.6632718598036517,
      "eval_f1_macro": 0.6609340561978665,
      "eval_loss": 0.8419658541679382,
      "eval_runtime": 167.688,
      "eval_samples_per_second": 64.996,
      "eval_steps_per_second": 4.067,
      "step": 8298
    },
    {
      "epoch": 3.000723065798988,
      "grad_norm": 2.307487964630127,
      "learning_rate": 2.0841074566909365e-05,
      "loss": 0.4107,
      "step": 8300
    },
    {
      "epoch": 3.0187997107736804,
      "grad_norm": 5.037658214569092,
      "learning_rate": 2.076575445643987e-05,
      "loss": 0.4012,
      "step": 8350
    },
    {
      "epoch": 3.036876355748373,
      "grad_norm": 3.2059853076934814,
      "learning_rate": 2.0690434345970374e-05,
      "loss": 0.4055,
      "step": 8400
    },
    {
      "epoch": 3.054953000723066,
      "grad_norm": 3.831878423690796,
      "learning_rate": 2.061511423550088e-05,
      "loss": 0.3931,
      "step": 8450
    },
    {
      "epoch": 3.0730296456977584,
      "grad_norm": 8.77486515045166,
      "learning_rate": 2.0539794125031383e-05,
      "loss": 0.4228,
      "step": 8500
    },
    {
      "epoch": 3.0911062906724514,
      "grad_norm": 4.089517116546631,
      "learning_rate": 2.0464474014561888e-05,
      "loss": 0.3969,
      "step": 8550
    },
    {
      "epoch": 3.109182935647144,
      "grad_norm": 4.559712886810303,
      "learning_rate": 2.0389153904092393e-05,
      "loss": 0.402,
      "step": 8600
    },
    {
      "epoch": 3.1272595806218364,
      "grad_norm": 2.0901358127593994,
      "learning_rate": 2.0313833793622897e-05,
      "loss": 0.3936,
      "step": 8650
    },
    {
      "epoch": 3.1453362255965294,
      "grad_norm": 7.143661022186279,
      "learning_rate": 2.0238513683153402e-05,
      "loss": 0.3872,
      "step": 8700
    },
    {
      "epoch": 3.163412870571222,
      "grad_norm": 8.19290828704834,
      "learning_rate": 2.0163193572683907e-05,
      "loss": 0.3843,
      "step": 8750
    },
    {
      "epoch": 3.181489515545915,
      "grad_norm": 3.370347738265991,
      "learning_rate": 2.0087873462214415e-05,
      "loss": 0.4087,
      "step": 8800
    },
    {
      "epoch": 3.1995661605206074,
      "grad_norm": 3.767430067062378,
      "learning_rate": 2.0012553351744916e-05,
      "loss": 0.4024,
      "step": 8850
    },
    {
      "epoch": 3.2176428054953,
      "grad_norm": 3.7064096927642822,
      "learning_rate": 1.993723324127542e-05,
      "loss": 0.3923,
      "step": 8900
    },
    {
      "epoch": 3.235719450469993,
      "grad_norm": 3.142019033432007,
      "learning_rate": 1.9861913130805925e-05,
      "loss": 0.4,
      "step": 8950
    },
    {
      "epoch": 3.2537960954446854,
      "grad_norm": 2.5104494094848633,
      "learning_rate": 1.978659302033643e-05,
      "loss": 0.4124,
      "step": 9000
    },
    {
      "epoch": 3.2718727404193784,
      "grad_norm": 6.930845737457275,
      "learning_rate": 1.9711272909866938e-05,
      "loss": 0.3969,
      "step": 9050
    },
    {
      "epoch": 3.289949385394071,
      "grad_norm": 2.381883144378662,
      "learning_rate": 1.963595279939744e-05,
      "loss": 0.3937,
      "step": 9100
    },
    {
      "epoch": 3.3080260303687634,
      "grad_norm": 5.938523292541504,
      "learning_rate": 1.9560632688927944e-05,
      "loss": 0.4094,
      "step": 9150
    },
    {
      "epoch": 3.3261026753434564,
      "grad_norm": 5.223520755767822,
      "learning_rate": 1.9485312578458448e-05,
      "loss": 0.4078,
      "step": 9200
    },
    {
      "epoch": 3.344179320318149,
      "grad_norm": 1.828596830368042,
      "learning_rate": 1.9409992467988953e-05,
      "loss": 0.3871,
      "step": 9250
    },
    {
      "epoch": 3.3622559652928414,
      "grad_norm": 6.271387100219727,
      "learning_rate": 1.933467235751946e-05,
      "loss": 0.3942,
      "step": 9300
    },
    {
      "epoch": 3.3803326102675344,
      "grad_norm": 7.902575492858887,
      "learning_rate": 1.9259352247049962e-05,
      "loss": 0.3813,
      "step": 9350
    },
    {
      "epoch": 3.398409255242227,
      "grad_norm": 4.555846691131592,
      "learning_rate": 1.9184032136580467e-05,
      "loss": 0.3914,
      "step": 9400
    },
    {
      "epoch": 3.41648590021692,
      "grad_norm": 2.902078866958618,
      "learning_rate": 1.910871202611097e-05,
      "loss": 0.4049,
      "step": 9450
    },
    {
      "epoch": 3.4345625451916124,
      "grad_norm": 3.0546140670776367,
      "learning_rate": 1.9033391915641476e-05,
      "loss": 0.3922,
      "step": 9500
    },
    {
      "epoch": 3.4526391901663054,
      "grad_norm": 5.814784049987793,
      "learning_rate": 1.8958071805171984e-05,
      "loss": 0.4102,
      "step": 9550
    },
    {
      "epoch": 3.470715835140998,
      "grad_norm": 3.7263131141662598,
      "learning_rate": 1.8882751694702485e-05,
      "loss": 0.404,
      "step": 9600
    },
    {
      "epoch": 3.4887924801156904,
      "grad_norm": 5.130584716796875,
      "learning_rate": 1.880743158423299e-05,
      "loss": 0.3946,
      "step": 9650
    },
    {
      "epoch": 3.5068691250903834,
      "grad_norm": 3.632659435272217,
      "learning_rate": 1.8733617875972884e-05,
      "loss": 0.3976,
      "step": 9700
    },
    {
      "epoch": 3.524945770065076,
      "grad_norm": 4.093955993652344,
      "learning_rate": 1.8658297765503393e-05,
      "loss": 0.3781,
      "step": 9750
    },
    {
      "epoch": 3.5430224150397684,
      "grad_norm": 10.769469261169434,
      "learning_rate": 1.8582977655033894e-05,
      "loss": 0.4004,
      "step": 9800
    },
    {
      "epoch": 3.5610990600144614,
      "grad_norm": 6.549926280975342,
      "learning_rate": 1.85076575445644e-05,
      "loss": 0.3686,
      "step": 9850
    },
    {
      "epoch": 3.579175704989154,
      "grad_norm": 4.235777854919434,
      "learning_rate": 1.8432337434094903e-05,
      "loss": 0.394,
      "step": 9900
    },
    {
      "epoch": 3.5972523499638465,
      "grad_norm": 5.064091205596924,
      "learning_rate": 1.8357017323625408e-05,
      "loss": 0.3773,
      "step": 9950
    },
    {
      "epoch": 3.6153289949385394,
      "grad_norm": 3.2539026737213135,
      "learning_rate": 1.8281697213155916e-05,
      "loss": 0.3993,
      "step": 10000
    },
    {
      "epoch": 3.633405639913232,
      "grad_norm": 4.198215484619141,
      "learning_rate": 1.8206377102686417e-05,
      "loss": 0.3912,
      "step": 10050
    },
    {
      "epoch": 3.651482284887925,
      "grad_norm": 4.208102226257324,
      "learning_rate": 1.813105699221692e-05,
      "loss": 0.4024,
      "step": 10100
    },
    {
      "epoch": 3.6695589298626174,
      "grad_norm": 2.8826558589935303,
      "learning_rate": 1.8055736881747426e-05,
      "loss": 0.4049,
      "step": 10150
    },
    {
      "epoch": 3.6876355748373104,
      "grad_norm": 10.679771423339844,
      "learning_rate": 1.798041677127793e-05,
      "loss": 0.3869,
      "step": 10200
    },
    {
      "epoch": 3.705712219812003,
      "grad_norm": 5.797079563140869,
      "learning_rate": 1.790509666080844e-05,
      "loss": 0.395,
      "step": 10250
    },
    {
      "epoch": 3.7237888647866955,
      "grad_norm": 3.2002053260803223,
      "learning_rate": 1.782977655033894e-05,
      "loss": 0.3958,
      "step": 10300
    },
    {
      "epoch": 3.7418655097613884,
      "grad_norm": 5.545964241027832,
      "learning_rate": 1.7754456439869445e-05,
      "loss": 0.4019,
      "step": 10350
    },
    {
      "epoch": 3.759942154736081,
      "grad_norm": 3.2150893211364746,
      "learning_rate": 1.767913632939995e-05,
      "loss": 0.4005,
      "step": 10400
    },
    {
      "epoch": 3.7780187997107735,
      "grad_norm": 7.776553630828857,
      "learning_rate": 1.7603816218930454e-05,
      "loss": 0.3986,
      "step": 10450
    },
    {
      "epoch": 3.7960954446854664,
      "grad_norm": 8.585441589355469,
      "learning_rate": 1.7528496108460962e-05,
      "loss": 0.3906,
      "step": 10500
    },
    {
      "epoch": 3.814172089660159,
      "grad_norm": 6.91293478012085,
      "learning_rate": 1.7453175997991463e-05,
      "loss": 0.3892,
      "step": 10550
    },
    {
      "epoch": 3.8322487346348515,
      "grad_norm": 2.904839038848877,
      "learning_rate": 1.7377855887521968e-05,
      "loss": 0.3866,
      "step": 10600
    },
    {
      "epoch": 3.8503253796095445,
      "grad_norm": 3.278684616088867,
      "learning_rate": 1.7302535777052472e-05,
      "loss": 0.3877,
      "step": 10650
    },
    {
      "epoch": 3.8684020245842374,
      "grad_norm": 9.626919746398926,
      "learning_rate": 1.7227215666582977e-05,
      "loss": 0.3977,
      "step": 10700
    },
    {
      "epoch": 3.88647866955893,
      "grad_norm": 3.8355517387390137,
      "learning_rate": 1.7151895556113485e-05,
      "loss": 0.3872,
      "step": 10750
    },
    {
      "epoch": 3.9045553145336225,
      "grad_norm": 25.27735710144043,
      "learning_rate": 1.7076575445643986e-05,
      "loss": 0.3974,
      "step": 10800
    },
    {
      "epoch": 3.9226319595083154,
      "grad_norm": 8.622859954833984,
      "learning_rate": 1.700125533517449e-05,
      "loss": 0.4019,
      "step": 10850
    },
    {
      "epoch": 3.940708604483008,
      "grad_norm": 4.412594318389893,
      "learning_rate": 1.6925935224704996e-05,
      "loss": 0.4044,
      "step": 10900
    },
    {
      "epoch": 3.9587852494577005,
      "grad_norm": 11.546675682067871,
      "learning_rate": 1.68506151142355e-05,
      "loss": 0.3879,
      "step": 10950
    },
    {
      "epoch": 3.9768618944323935,
      "grad_norm": 8.539558410644531,
      "learning_rate": 1.6775295003766008e-05,
      "loss": 0.3897,
      "step": 11000
    },
    {
      "epoch": 3.994938539407086,
      "grad_norm": 6.71378755569458,
      "learning_rate": 1.669997489329651e-05,
      "loss": 0.3864,
      "step": 11050
    },
    {
      "epoch": 4.0,
      "eval_accuracy": 0.6832736948343885,
      "eval_f1_macro": 0.6793805317275947,
      "eval_loss": 0.8750564455986023,
      "eval_runtime": 167.693,
      "eval_samples_per_second": 64.994,
      "eval_steps_per_second": 4.067,
      "step": 11064
    }
  ],
  "logging_steps": 50,
  "max_steps": 22128,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 8,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 9.315078559543296e+16,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
